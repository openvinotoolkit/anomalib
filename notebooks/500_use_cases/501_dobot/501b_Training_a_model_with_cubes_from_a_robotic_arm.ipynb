{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of production line with defects - Training Via Configuration File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will train a Anomalib model using the Anomalib API and our own dataset. This notebook is also part of the Dobot series notebooks.\n",
    "\n",
    "### Use case\n",
    "\n",
    "Using the [Dobot Magician](https://www.dobot.cc/dobot-magician/product-overview.html) we could simulate a production line system. Imagine we have a cubes factory and they need to know when a defect piece appear in the process. We know very well what is the aspecto of the normal cubes. Defects are coming no often and we need to put those defect cubes out of the production line.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/10940214/174126337-b344bbdc-6343-4d85-93e8-0cb1bf39a4e3.png\" alt=\"drawing\" style=\"width:400px;\"/>\n",
    "\n",
    "\n",
    "| Class | Yellow cube | Red cube | Green cube | Inferencing using Anomalib\n",
    "| --------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ | --------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| Normal | <img src=\"https://user-images.githubusercontent.com/10940214/174083561-38eec918-efc2-4ceb-99b1-bbb4c91396b2.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174083638-85ff889c-6222-4428-9c7d-9ad62bd15afe.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174083707-364177d4-373b-4891-96ce-3e5ea923e440.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174129305-03d9b71c-dfd9-492f-b42e-01c5c24171cc.jpg\" alt=\"drawing\" style=\"width:150px;\"/> |\n",
    "| Abnormal | <img src=\"https://user-images.githubusercontent.com/10940214/174083805-df0a0b03-58c7-4ba8-af50-fd94d3a13e58.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174083873-22699523-22b4-4a55-a3da-6520095af8af.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174083944-38d5a6f4-f647-455b-ba4e-69482dfa3562.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | <img src=\"https://user-images.githubusercontent.com/10940214/174129253-f7a567d0-84f7-4050-8065-f00ba8bb973d.jpg\" alt=\"drawing\" style=\"width:150px;\"/> | \n",
    "\n",
    "Using Anomalib we are expecting to see this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the working directory and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from git.repo import Repo\n",
    "\n",
    "current_directory = Path.cwd()\n",
    "if current_directory.parent.name == \"500_use_cases\" and current_directory.name == \"501_dobot\":\n",
    "    # On the assumption that, the notebook is located in\n",
    "    #   ~/anomalib/notebooks/500_use_cases/501_dobot\n",
    "    root_directory = current_directory.parent.parent.parent\n",
    "elif current_directory.name == \"anomalib\":\n",
    "    # This means that the notebook is run from the main anomalib directory.\n",
    "    root_directory = current_directory\n",
    "else:\n",
    "    # Otherwise, we'll need to clone the anomalib repo to the `current_directory`\n",
    "    repo = Repo.clone_from(url=\"https://github.com/openvinotoolkit/anomalib.git\", to_path=current_directory)\n",
    "    root_directory = current_directory / \"anomalib\"\n",
    "\n",
    "os.chdir(root_directory)\n",
    "notebook_path = root_directory / \"notebooks\" / \"500_use_cases\" / \"501_dobot\"\n",
    "dataset_path = root_directory / \"datasets\" / \"cubes\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data.utils import read_image\n",
    "from anomalib.deploy import OpenVINOInferencer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration - Via API\n",
    "\n",
    "Using the API we will setup different Anomalib modules, data manager, model manager, experiment manager and callback manager. We can modify the setup modifying the configuration file and also API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Currently, there are **13** anomaly detection models available in `anomalib` library. Namely,\n",
    "\n",
    "- [CFA](https://arxiv.org/abs/2206.04325)\n",
    "- [CS-Flow](https://arxiv.org/abs/2110.02855v1)\n",
    "- [CFlow](https://arxiv.org/pdf/2107.12571v1.pdf)\n",
    "- [DFKDE](https://github.com/openvinotoolkit/anomalib/tree/main/anomalib/models/dfkde)\n",
    "- [DFM](https://arxiv.org/pdf/1909.11786.pdf)\n",
    "- [DRAEM](https://arxiv.org/abs/2108.07610)\n",
    "- [FastFlow](https://arxiv.org/abs/2111.07677)\n",
    "- [Ganomaly](https://arxiv.org/abs/1805.06725)\n",
    "- [Padim](https://arxiv.org/pdf/2011.08785.pdf)\n",
    "- [Patchcore](https://arxiv.org/pdf/2106.08265.pdf)\n",
    "- [Reverse Distillation](https://arxiv.org/abs/2201.10703)\n",
    "- [R-KDE](https://ieeexplore.ieee.org/document/8999287)\n",
    "- [STFPM](https://arxiv.org/pdf/2103.04257.pdf)\n",
    "\n",
    "In this tutorial, we'll be using Padim. Now, let's get their config paths from the respected folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"padim\"  # 'padim', 'cflow', 'stfpm', 'ganomaly', 'dfkde', 'patchcore'\n",
    "CONFIG_PATH = notebook_path / \"cubes_config.yaml\"\n",
    "with open(file=CONFIG_PATH, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use [get_configurable_parameter](https://github.com/openvinotoolkit/anomalib/blob/development/anomalib/config/config.py#L114) function to read the configs from the path and return them in a dictionary. We use the default config file that comes with Padim implementation, which uses `./datasets/cubes` as the path to the dataset. We need to overwrite this after loading the config.\n",
    "\n",
    "Now, the config file is updated as we want. We can now start model training with it. Here we will be using datamodule, model and callbacks to train the model. Callbacks are self-contained objects, which contains non-essential logic. This way we could inject as many callbacks as possible such as ModelLoading, Timer, Metrics, Normalization and Visualization\n",
    "\n",
    "In addition to the training, we would like to perform inference using OpenVINO. Therefore we will set the export configuration to openvino so that anomalib would export the trained model to the openvino format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the config file to model, logger, callbacks and datamodule\n",
    "config = get_configurable_parameters(config_path=CONFIG_PATH)\n",
    "config[\"dataset\"][\"path\"] = str(dataset_path)  # or wherever the Custom Dataset is stored.\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Cubes\n",
    "\n",
    "Prepare your own dataset for normal and defect pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.data.folder import Folder\n",
    "from anomalib.data.task_type import TaskType\n",
    "\n",
    "datamodule = Folder(\n",
    "    root=dataset_path,\n",
    "    normal_dir=\"normal\",\n",
    "    abnormal_dir=\"abnormal\",\n",
    "    normal_split_ratio=0.2,\n",
    "    image_size=(256, 256),\n",
    "    train_batch_size=32,\n",
    "    eval_batch_size=32,\n",
    "    task=TaskType.CLASSIFICATION,\n",
    ")\n",
    "datamodule.setup()  # Split the data to train/val/test/prediction sets.\n",
    "datamodule.prepare_data()  # Create train/val/test/predic dataloaders\n",
    "\n",
    "i, data = next(enumerate(datamodule.val_dataloader()))\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image size\n",
    "print(data[\"image\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Model \n",
    "We will use Padim model for this use case, which could imported from `anomalib.models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.models import Padim\n",
    "\n",
    "model = Padim(\n",
    "    input_size=(256, 256),\n",
    "    backbone=\"resnet18\",\n",
    "    layers=[\"layer1\", \"layer2\", \"layer3\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Callbacks\n",
    "\n",
    "To train the model properly, we will to add some other \"non-essential\" logic such as saving the weights, early-stopping, normalizing the anomaly scores and visualizing the input/output images. To achieve these we use `Callbacks`. Anomalib has its own callbacks and also supports PyTorch Lightning's native callbacks. So, let's create the list of callbacks we want to execute during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.post_processing import NormalizationMethod, ThresholdMethod\n",
    "from anomalib.utils.callbacks import (\n",
    "    MetricsConfigurationCallback,\n",
    "    MinMaxNormalizationCallback,\n",
    "    PostProcessingConfigurationCallback,\n",
    ")\n",
    "from anomalib.utils.callbacks.export import ExportCallback, ExportMode\n",
    "\n",
    "callbacks = [\n",
    "    MetricsConfigurationCallback(\n",
    "        task=TaskType.CLASSIFICATION,\n",
    "        image_metrics=[\"AUROC\"],\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        mode=\"max\",\n",
    "        monitor=\"image_AUROC\",\n",
    "    ),\n",
    "    PostProcessingConfigurationCallback(\n",
    "        normalization_method=NormalizationMethod.MIN_MAX,\n",
    "        threshold_method=ThresholdMethod.ADAPTIVE,\n",
    "    ),\n",
    "    MinMaxNormalizationCallback(),\n",
    "    ExportCallback(\n",
    "        input_size=(256, 256),\n",
    "        dirpath=str(notebook_path),\n",
    "        filename=\"model\",\n",
    "        export_mode=ExportMode.OPENVINO,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we set up the datamodule, model and the callbacks, we could now train the model.\n",
    "\n",
    "The final component to train the model is `pytorch_lightning` `Trainer` object, which handles train/test/predict pipeline. Let's create the trainer object to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"auto\",\n",
    "    auto_scale_batch_size=False,\n",
    "    check_val_every_n_epoch=1,\n",
    "    devices=1,\n",
    "    gpus=None,\n",
    "    max_epochs=1,\n",
    "    num_sanity_val_steps=0,\n",
    "    val_check_interval=1.0,\n",
    ")\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "test_results = trainer.test(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO Inference\n",
    "\n",
    "Now that we trained and tested a model, we could check a single inference result using OpenVINO inferencer object. This will demonstrate how a trained model could be used for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Test Image\n",
    "Let's read an image from the test set and perform inference using OpenVINO inferencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = root_directory / \"datasets/cubes/abnormal/input_20230210134059.jpg\"\n",
    "image = read_image(path=\"datasets/cubes/abnormal/input_20230210134059.jpg\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the OpenVINO Model\n",
    "\n",
    "By default, the output files are saved into `results` directory. Let's check where the OpenVINO model is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "openvino_model_path = notebook_path / \"openvino\" / \"model.bin\"\n",
    "metadata_path = notebook_path / \"openvino\" / \"meta_data.json\"\n",
    "print(openvino_model_path.exists(), metadata_path.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = OpenVINOInferencer(\n",
    "    config=CONFIG_PATH,  # Pass the config file to the inferencer.\n",
    "    path=openvino_model_path,  # Path to the OpenVINO IR model.\n",
    "    meta_data_path=metadata_path,  # Path to the metadata file.\n",
    "    device=\"CPU\",  # We would like to run it on an Intel CPU.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Inference\n",
    "Predicting an image using OpenVINO inferencer is as simple as calling `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)\n",
    "predictions = inferencer.predict(image=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `predictions` contain any relevant information regarding the task type. For example, predictions for a segmentation model could contain image, anomaly maps, predicted scores, labels or masks.\n",
    "### Visualizing Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "\n",
    "# setting values to rows and column variables\n",
    "rows = 1\n",
    "columns = 5\n",
    "\n",
    "# Adds a subplot at the 1st position\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "\n",
    "# showing image\n",
    "plt.imshow(predictions.image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Original\")\n",
    "\n",
    "# Adds a subplot at the 2nd position\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "\n",
    "# showing image\n",
    "plt.imshow(predictions.anomaly_map)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Anomaly Map\")\n",
    "\n",
    "# Adds a subplot at the 3rd position\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "\n",
    "# showing image\n",
    "plt.imshow(predictions.heat_map)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Heat Map\")\n",
    "\n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "\n",
    "# showing image\n",
    "plt.imshow(predictions.pred_mask)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Pred. Mask\")\n",
    "\n",
    "# Adds a subplot at the 4th position\n",
    "fig.add_subplot(rows, columns, 5)\n",
    "\n",
    "# showing image\n",
    "plt.imshow(predictions.segmentations)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Result\")\n",
    "\n",
    "print(predictions.pred_score, predictions.pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_anomalib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "18f8999b3d132acda9ed72c7f0f7e54d3c533278cffbadac58c30769cf876377"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
